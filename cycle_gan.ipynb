{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CYCLE GAN TRAINING!\n",
    "\n",
    "The purpose is transfer the stylel of impressionist paintings into paitnings of people in operating roomm\n",
    "\n",
    "Hidden incentive: some friends are graduating and it'd a cool gift.... if it works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Had to learn about CycleGANs since we do not cover this in class, so here is what I learned at a basic concept:\n",
    "\n",
    "We did learn about generator and discriminators and so this is basically that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4000 training paintings\n",
      "Found 240 training OR images\n",
      "Found 1000 test paintings\n",
      "Found 54 test OR images\n",
      "Training CycleGAN for 2 epochs...\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 534\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(test_or_images)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m test OR images\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    533\u001b[0m \u001b[38;5;66;03m# Run CycleGAN\u001b[39;00m\n\u001b[0;32m--> 534\u001b[0m generator_g, generator_f, explorer \u001b[38;5;241m=\u001b[39m run_cyclegan(\n\u001b[1;32m    535\u001b[0m     train_paintings, \n\u001b[1;32m    536\u001b[0m     train_or_images, \n\u001b[1;32m    537\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    538\u001b[0m )\n",
      "Cell \u001b[0;32mIn[2], line 498\u001b[0m, in \u001b[0;36mrun_cyclegan\u001b[0;34m(train_paintings, train_or_images, epochs)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining CycleGAN for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m epochs...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 498\u001b[0m generator_g, generator_f \u001b[38;5;241m=\u001b[39m train_cyclegan(\n\u001b[1;32m    499\u001b[0m     train_paintings_ds, train_or_images_ds, \n\u001b[1;32m    500\u001b[0m     epochs\u001b[38;5;241m=\u001b[39mepochs\n\u001b[1;32m    501\u001b[0m )\n\u001b[1;32m    503\u001b[0m \u001b[38;5;66;03m# Create latent space explorer\u001b[39;00m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExploring latent space:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 323\u001b[0m, in \u001b[0;36mtrain_cyclegan\u001b[0;34m(train_x_dataset, train_y_dataset, epochs, patience)\u001b[0m\n\u001b[1;32m    320\u001b[0m total_disc_y_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mzip((train_x_dataset, train_y_dataset)):\n\u001b[0;32m--> 323\u001b[0m     losses \u001b[38;5;241m=\u001b[39m train_step(\n\u001b[1;32m    324\u001b[0m         x_batch, y_batch,\n\u001b[1;32m    325\u001b[0m         generator_g, generator_f,\n\u001b[1;32m    326\u001b[0m         discriminator_x, discriminator_y,\n\u001b[1;32m    327\u001b[0m         generator_g_optimizer, generator_f_optimizer,\n\u001b[1;32m    328\u001b[0m         discriminator_x_optimizer, discriminator_y_optimizer\n\u001b[1;32m    329\u001b[0m     )\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;66;03m# Update loss totals\u001b[39;00m\n\u001b[1;32m    332\u001b[0m     total_gen_g_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m losses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgen_g_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[0;32mIn[2], line 411\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(real_x, real_y, generator_g, generator_f, discriminator_x, discriminator_y, generator_g_optimizer, generator_f_optimizer, discriminator_x_optimizer, discriminator_y_optimizer)\u001b[0m\n\u001b[1;32m    409\u001b[0m disc_real_x \u001b[38;5;241m=\u001b[39m discriminator_x(real_x, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    410\u001b[0m disc_real_y \u001b[38;5;241m=\u001b[39m discriminator_y(real_y, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 411\u001b[0m disc_fake_x \u001b[38;5;241m=\u001b[39m discriminator_x(fake_x, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    412\u001b[0m disc_fake_y \u001b[38;5;241m=\u001b[39m discriminator_y(fake_y, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    414\u001b[0m \u001b[38;5;66;03m# Generator losses\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/layer.py:899\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    897\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 899\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    900\u001b[0m \u001b[38;5;66;03m# Change the layout for the layer output if needed.\u001b[39;00m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;66;03m# This is useful for relayout intermediate tensor in the model\u001b[39;00m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;66;03m# to achieve the optimal performance.\u001b[39;00m\n\u001b[1;32m    903\u001b[0m distribution \u001b[38;5;241m=\u001b[39m distribution_lib\u001b[38;5;241m.\u001b[39mdistribution()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras/src/ops/operation.py:46\u001b[0m, in \u001b[0;36mOperation.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m             call_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall\n\u001b[1;32m     42\u001b[0m     call_fn \u001b[38;5;241m=\u001b[39m traceback_utils\u001b[38;5;241m.\u001b[39minject_argument_info_in_traceback(\n\u001b[1;32m     43\u001b[0m         call_fn,\n\u001b[1;32m     44\u001b[0m         object_name\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.call()\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     45\u001b[0m     )\n\u001b[0;32m---> 46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m call_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Plain flow.\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors(args, kwargs):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:156\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras/src/models/functional.py:182\u001b[0m, in \u001b[0;36mFunctional.call\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    181\u001b[0m             backend\u001b[38;5;241m.\u001b[39mset_keras_mask(x, mask)\n\u001b[0;32m--> 182\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_through_graph(\n\u001b[1;32m    183\u001b[0m     inputs, operation_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m op: operation_fn(op, training\u001b[38;5;241m=\u001b[39mtraining)\n\u001b[1;32m    184\u001b[0m )\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unpack_singleton(outputs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras/src/ops/function.py:171\u001b[0m, in \u001b[0;36mFunction._run_through_graph\u001b[0;34m(self, inputs, operation_fn, call_fn)\u001b[0m\n\u001b[1;32m    169\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m call_fn(op, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 171\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m op(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# Update tensor_dict.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(node\u001b[38;5;241m.\u001b[39moutputs, tree\u001b[38;5;241m.\u001b[39mflatten(outputs)):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras/src/models/functional.py:632\u001b[0m, in \u001b[0;36moperation_fn.<locals>.call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28mhasattr\u001b[39m(operation, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_call_has_training_arg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m operation\u001b[38;5;241m.\u001b[39m_call_has_training_arg\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m training \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    630\u001b[0m ):\n\u001b[1;32m    631\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m training\n\u001b[0;32m--> 632\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m operation(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/layer.py:899\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    897\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 899\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    900\u001b[0m \u001b[38;5;66;03m# Change the layout for the layer output if needed.\u001b[39;00m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;66;03m# This is useful for relayout intermediate tensor in the model\u001b[39;00m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;66;03m# to achieve the optimal performance.\u001b[39;00m\n\u001b[1;32m    903\u001b[0m distribution \u001b[38;5;241m=\u001b[39m distribution_lib\u001b[38;5;241m.\u001b[39mdistribution()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras/src/ops/operation.py:46\u001b[0m, in \u001b[0;36mOperation.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m             call_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall\n\u001b[1;32m     42\u001b[0m     call_fn \u001b[38;5;241m=\u001b[39m traceback_utils\u001b[38;5;241m.\u001b[39minject_argument_info_in_traceback(\n\u001b[1;32m     43\u001b[0m         call_fn,\n\u001b[1;32m     44\u001b[0m         object_name\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.call()\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     45\u001b[0m     )\n\u001b[0;32m---> 46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m call_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Plain flow.\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors(args, kwargs):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:156\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:243\u001b[0m, in \u001b[0;36mBaseConv.call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[0;32m--> 243\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvolution_op(\n\u001b[1;32m    244\u001b[0m         inputs,\n\u001b[1;32m    245\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel,\n\u001b[1;32m    246\u001b[0m     )\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_bias:\n\u001b[1;32m    248\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_format \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchannels_last\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:233\u001b[0m, in \u001b[0;36mBaseConv.convolution_op\u001b[0;34m(self, inputs, kernel)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvolution_op\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, kernel):\n\u001b[0;32m--> 233\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mconv(\n\u001b[1;32m    234\u001b[0m         inputs,\n\u001b[1;32m    235\u001b[0m         kernel,\n\u001b[1;32m    236\u001b[0m         strides\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrides),\n\u001b[1;32m    237\u001b[0m         padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding,\n\u001b[1;32m    238\u001b[0m         dilation_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation_rate,\n\u001b[1;32m    239\u001b[0m         data_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_format,\n\u001b[1;32m    240\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras/src/ops/nn.py:1183\u001b[0m, in \u001b[0;36mconv\u001b[0;34m(inputs, kernel, strides, padding, data_format, dilation_rate)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors((inputs,)):\n\u001b[1;32m   1180\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Conv(strides, padding, data_format, dilation_rate)\u001b[38;5;241m.\u001b[39msymbolic_call(\n\u001b[1;32m   1181\u001b[0m         inputs, kernel\n\u001b[1;32m   1182\u001b[0m     )\n\u001b[0;32m-> 1183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mconv(\n\u001b[1;32m   1184\u001b[0m     inputs, kernel, strides, padding, data_format, dilation_rate\n\u001b[1;32m   1185\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras/src/backend/tensorflow/nn.py:301\u001b[0m, in \u001b[0;36mconv\u001b[0;34m(inputs, kernel, strides, padding, data_format, dilation_rate)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _conv_xla()\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _conv()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras/src/backend/tensorflow/nn.py:274\u001b[0m, in \u001b[0;36mconv.<locals>._conv\u001b[0;34m()\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_conv\u001b[39m():\n\u001b[1;32m    273\u001b[0m     tf_data_format \u001b[38;5;241m=\u001b[39m _convert_data_format(data_format, \u001b[38;5;28mlen\u001b[39m(inputs\u001b[38;5;241m.\u001b[39mshape))\n\u001b[0;32m--> 274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mconvolution(\n\u001b[1;32m    275\u001b[0m         inputs,\n\u001b[1;32m    276\u001b[0m         kernel,\n\u001b[1;32m    277\u001b[0m         strides,\n\u001b[1;32m    278\u001b[0m         padding\u001b[38;5;241m.\u001b[39mupper(),\n\u001b[1;32m    279\u001b[0m         data_format\u001b[38;5;241m=\u001b[39mtf_data_format,\n\u001b[1;32m    280\u001b[0m         dilations\u001b[38;5;241m=\u001b[39mdilation_rate,\n\u001b[1;32m    281\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/util/dispatch.py:1260\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1258\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1259\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1260\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_target(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   1262\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1263\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1264\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/ops/nn_ops.py:1186\u001b[0m, in \u001b[0;36mconvolution_v2\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1176\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnn.convolution\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;129m@dispatch\u001b[39m\u001b[38;5;241m.\u001b[39madd_dispatch_support\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvolution_v2\u001b[39m(  \u001b[38;5;66;03m# pylint: disable=missing-docstring\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1184\u001b[0m     dilations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1185\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m-> 1186\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m convolution_internal(\n\u001b[1;32m   1187\u001b[0m       \u001b[38;5;28minput\u001b[39m,  \u001b[38;5;66;03m# pylint: disable=redefined-builtin\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m       filters,\n\u001b[1;32m   1189\u001b[0m       strides\u001b[38;5;241m=\u001b[39mstrides,\n\u001b[1;32m   1190\u001b[0m       padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   1191\u001b[0m       data_format\u001b[38;5;241m=\u001b[39mdata_format,\n\u001b[1;32m   1192\u001b[0m       dilations\u001b[38;5;241m=\u001b[39mdilations,\n\u001b[1;32m   1193\u001b[0m       name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/ops/nn_ops.py:1319\u001b[0m, in \u001b[0;36mconvolution_internal\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name, call_from_convolution, num_spatial_dims)\u001b[0m\n\u001b[1;32m   1316\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1317\u001b[0m     op \u001b[38;5;241m=\u001b[39m conv1d\n\u001b[0;32m-> 1319\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m op(\n\u001b[1;32m   1320\u001b[0m       \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1321\u001b[0m       filters,\n\u001b[1;32m   1322\u001b[0m       strides,\n\u001b[1;32m   1323\u001b[0m       padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   1324\u001b[0m       data_format\u001b[38;5;241m=\u001b[39mdata_format,\n\u001b[1;32m   1325\u001b[0m       dilations\u001b[38;5;241m=\u001b[39mdilations,\n\u001b[1;32m   1326\u001b[0m       name\u001b[38;5;241m=\u001b[39mname)\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1328\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m channel_index \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/ops/nn_ops.py:2793\u001b[0m, in \u001b[0;36m_conv2d_expanded_batch\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   2789\u001b[0m input_rank \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mrank\n\u001b[1;32m   2790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_rank \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m input_rank \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m5\u001b[39m:\n\u001b[1;32m   2791\u001b[0m   \u001b[38;5;66;03m# We avoid calling squeeze_batch_dims to reduce extra python function\u001b[39;00m\n\u001b[1;32m   2792\u001b[0m   \u001b[38;5;66;03m# call slowdown in eager mode.  This branch doesn't require reshapes.\u001b[39;00m\n\u001b[0;32m-> 2793\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m gen_nn_ops\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m   2794\u001b[0m       \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   2795\u001b[0m       \u001b[38;5;28mfilter\u001b[39m\u001b[38;5;241m=\u001b[39mfilters,\n\u001b[1;32m   2796\u001b[0m       strides\u001b[38;5;241m=\u001b[39mstrides,\n\u001b[1;32m   2797\u001b[0m       padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   2798\u001b[0m       data_format\u001b[38;5;241m=\u001b[39mdata_format,\n\u001b[1;32m   2799\u001b[0m       dilations\u001b[38;5;241m=\u001b[39mdilations,\n\u001b[1;32m   2800\u001b[0m       name\u001b[38;5;241m=\u001b[39mname)\n\u001b[1;32m   2801\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m squeeze_batch_dims(\n\u001b[1;32m   2802\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   2803\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2810\u001b[0m     inner_rank\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m   2811\u001b[0m     name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/ops/gen_nn_ops.py:1345\u001b[0m, in \u001b[0;36mconv2d\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1343\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1345\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m conv2d_eager_fallback(\n\u001b[1;32m   1346\u001b[0m       \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mfilter\u001b[39m, strides\u001b[38;5;241m=\u001b[39mstrides, use_cudnn_on_gpu\u001b[38;5;241m=\u001b[39muse_cudnn_on_gpu,\n\u001b[1;32m   1347\u001b[0m       padding\u001b[38;5;241m=\u001b[39mpadding, explicit_paddings\u001b[38;5;241m=\u001b[39mexplicit_paddings,\n\u001b[1;32m   1348\u001b[0m       data_format\u001b[38;5;241m=\u001b[39mdata_format, dilations\u001b[38;5;241m=\u001b[39mdilations, name\u001b[38;5;241m=\u001b[39mname, ctx\u001b[38;5;241m=\u001b[39m_ctx)\n\u001b[1;32m   1349\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_SymbolicException:\n\u001b[1;32m   1350\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Add nodes to the TensorFlow graph.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/ops/gen_nn_ops.py:1434\u001b[0m, in \u001b[0;36mconv2d_eager_fallback\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name, ctx)\u001b[0m\n\u001b[1;32m   1430\u001b[0m _inputs_flat \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mfilter\u001b[39m]\n\u001b[1;32m   1431\u001b[0m _attrs \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m\"\u001b[39m, _attr_T, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrides\u001b[39m\u001b[38;5;124m\"\u001b[39m, strides, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cudnn_on_gpu\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1432\u001b[0m use_cudnn_on_gpu, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpadding\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexplicit_paddings\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1433\u001b[0m explicit_paddings, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_format\u001b[39m\u001b[38;5;124m\"\u001b[39m, data_format, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdilations\u001b[39m\u001b[38;5;124m\"\u001b[39m, dilations)\n\u001b[0;32m-> 1434\u001b[0m _result \u001b[38;5;241m=\u001b[39m _execute\u001b[38;5;241m.\u001b[39mexecute(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConv2D\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m, inputs\u001b[38;5;241m=\u001b[39m_inputs_flat, attrs\u001b[38;5;241m=\u001b[39m_attrs,\n\u001b[1;32m   1435\u001b[0m                            ctx\u001b[38;5;241m=\u001b[39mctx, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[1;32m   1436\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _execute\u001b[38;5;241m.\u001b[39mmust_record_gradient():\n\u001b[1;32m   1437\u001b[0m   _execute\u001b[38;5;241m.\u001b[39mrecord_gradient(\n\u001b[1;32m   1438\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConv2D\u001b[39m\u001b[38;5;124m\"\u001b[39m, _inputs_flat, _attrs, _result)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "# Constants\n",
    "IMG_HEIGHT = 256\n",
    "IMG_WIDTH = 256\n",
    "OUTPUT_CHANNELS = 3\n",
    "BATCH_SIZE = 2 # played around with 1-3 trying to weight memory vs computational intensity \n",
    "LAMBDA = 8  # cycle consistency loss is weighted 8 times higher than the adversarial loss in your overall loss function. \n",
    "#I wanted to have some creative freedom so i found that decreasing lamda gave me that becaue you get less faithful reconstructions, but too low and the model woudl collapse\n",
    "\n",
    "\n",
    "##############################\n",
    "# UP and DOWNSAMPLING BLOCKS #\n",
    "##############################\n",
    "# Downsapling block - to reduce spatial dimensions, extracts features\n",
    "def downsample(filters, size, apply_norm=True):\n",
    "    \n",
    "    \"\"\"\n",
    "    filters - number of filters in the convolution\n",
    "    size - kernel size\n",
    "    apply_norm - whether to apply batch normalization\n",
    "    \"\"\"\n",
    "    \n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    result = keras.Sequential() # from Keras, this is a linear neural network\n",
    "    \n",
    "    #convolution layer - extracts features from the image\n",
    "    result.add(layers.Conv2D(filters, size, strides=2, padding='same',\n",
    "                             kernel_initializer=initializer, use_bias=False))\n",
    "    \"\"\"\n",
    "    padding - to pad the input so output dimensions are determined solely by stride\n",
    "    use_baias - set to False since using batch normalization\n",
    "    \"\"\"\n",
    "    \n",
    "    if apply_norm:\n",
    "        result.add(layers.BatchNormalization()) #applying batch normalization: preventing extreme feature extraction + stabilizes training\n",
    "    \n",
    "    result.add(layers.LeakyReLU(0.2)) #using LeakyReLu\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Upsampling block - to increase the spatial dimensions, reconstructs the image\n",
    "def upsample(filters, size, apply_dropout=False): #i tried dropout true, but it was looksing some details so i turned it off, but could be turned on for fun\n",
    "    \n",
    "    \"\"\"\n",
    "    filters- number of filters in the convolution\n",
    "    size - kernel size\n",
    "    apply_dropout - whether to apply dropout\n",
    "    \"\"\"\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    \n",
    "    result = keras.Sequential()\n",
    "    \n",
    "    # deconvolution layer - similar architecture as convolution layer, but transposed\n",
    "    result.add(layers.Conv2DTranspose(filters, size, strides=2, padding='same',\n",
    "                                      kernel_initializer=initializer, use_bias=False))\n",
    "    \n",
    "    result.add(layers.BatchNormalization())\n",
    "    \n",
    "    if apply_dropout:\n",
    "        result.add(layers.Dropout(0.5)) #forces the model to learn more robust features from a highly compressed representation and not relying on single features\n",
    "    \n",
    "    result.add(layers.ReLU())\n",
    "    \n",
    "    return result\n",
    "\n",
    "###############################\n",
    "# GENERATOR AND DISCRIMINATOR #\n",
    "###############################\n",
    "# Generator model - here is where i had to learn abut the U architecture of the cycleGAN - this buis is the architecture of the generator\n",
    "def build_generator():\n",
    "    \"\"\"Generator model\"\"\"\n",
    "    inputs = layers.Input(shape=[IMG_HEIGHT, IMG_WIDTH, 3])\n",
    "    \n",
    "    #DOWNSAMPLING, FOLLOWED BY UPSAMPLING IN MIRROR AS THE U ARCHITECTURE FOR A CYCLEGAN : U ARCHITECTURE PRESERVES SPATIAL DIMENSIONS\n",
    "    # Downsampling (output filter, kernel size, apply norm)\n",
    "    down_stack = [\n",
    "        downsample(64, 4, apply_norm=False),  # tensor shape output layer = (batch size (bs), height (128), width (128), number of filters (64))\n",
    "        downsample(128, 4),  # (bs, 64, 64, 128)\n",
    "        downsample(256, 4),  # (bs, 32, 32, 256)\n",
    "        downsample(512, 4),  # (bs, 16, 16, 512)\n",
    "        downsample(512, 4),  # (bs, 8, 8, 512)\n",
    "        downsample(512, 4),  # (bs, 4, 4, 512)\n",
    "    ]\n",
    "    # above 512 layers is to create a bottle neck - which helps with feature saturatin and memory constraints \n",
    "    \n",
    "    # Upsampling \n",
    "    up_stack = [\n",
    "        upsample(512, 4, apply_dropout=True),  # (bs, 8, 8, 512)\n",
    "        upsample(512, 4, apply_dropout=True),  # (bs, 16, 16, 512)\n",
    "        upsample(256, 4),  # (bs, 32, 32, 256)\n",
    "        upsample(128, 4),  # (bs, 64, 64, 128)\n",
    "        upsample(64, 4),  # (bs, 128, 128, 64)\n",
    "    ]\n",
    "    #inverse as above to create the U\n",
    "    \n",
    "    \n",
    "    #output layer\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    last = layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n",
    "                                 strides=2,\n",
    "                                 padding='same',\n",
    "                                 kernel_initializer=initializer,\n",
    "                                 activation='tanh')  # (bs, 256, 256, 3)\n",
    "    \n",
    "    x = inputs\n",
    "    \n",
    "    # Downsampling through the model for x image\n",
    "    skips = []\n",
    "    for down in down_stack:\n",
    "        x = down(x)\n",
    "        skips.append(x) #skip connections for the U architecture is something i learned about. skip connections help the model learn better by allowing gradients to flow through the network more easily\n",
    "    \n",
    "    skips = reversed(skips[:-1]) #reversed to match, but not the bottleneck one (-1)\n",
    "    \n",
    "    # Upsampling and establishing the skip connections\n",
    "    for up, skip in zip(up_stack, skips):\n",
    "        x = up(x)\n",
    "        x = layers.Concatenate()([x, skip])\n",
    "    \n",
    "    x = last(x)\n",
    "    \n",
    "    return keras.Model(inputs=inputs, outputs=x)\n",
    "\n",
    "# Discriminator model - this is the discriminator and it does not have a U architecture since we are trying to discriminate between the real and fake images\n",
    "#i used a patchGAN discriminator which takes patches of the images to try to discriminate instad of the whole image. I read this was better 1) computation 2) in translation since we are interested in context and texture \n",
    "def build_discriminator():\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    \n",
    "    inp = layers.Input(shape=[IMG_HEIGHT, IMG_WIDTH, 3], name='input_image')\n",
    "    \n",
    "    x = inp\n",
    "    \n",
    "    down1 = downsample(64, 4, False)(x)  # (bs, 128, 128, 64)\n",
    "    down2 = downsample(128, 4)(down1)  # (bs, 64, 64, 128)\n",
    "    down3 = downsample(256, 4)(down2)  # (bs, 32, 32, 256)\n",
    "    \n",
    "    zero_pad1 = layers.ZeroPadding2D()(down3)  # (bs, 34, 34, 256)\n",
    "    conv = layers.Conv2D(512, 4, strides=1,\n",
    "                        kernel_initializer=initializer,\n",
    "                        use_bias=False)(zero_pad1)  # (bs, 31, 31, 512)\n",
    "    \n",
    "    batchnorm1 = layers.BatchNormalization()(conv)\n",
    "    leaky_relu = layers.LeakyReLU()(batchnorm1)\n",
    "    \n",
    "    zero_pad2 = layers.ZeroPadding2D()(leaky_relu)  # (bs, 33, 33, 512)\n",
    "    last = layers.Conv2D(1, 4, strides=1,\n",
    "                        kernel_initializer=initializer)(zero_pad2)  # (bs, 30, 30, 1)\n",
    "    \n",
    "    return keras.Model(inputs=inp, outputs=last)\n",
    "\n",
    "##############################\n",
    "# LOSS FUNCTIONS #\n",
    "##############################\n",
    "#how well the generator is fooling the discriminator\n",
    "def generator_loss(generated_output):\n",
    "    loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True) #using binarycross entropy, not by probability but how far away form 1\n",
    "    return loss_object(tf.ones_like(generated_output), generated_output) \n",
    "\n",
    "#how well the discriminator can distinguish real from fake\n",
    "def discriminator_loss(real_output, generated_output): \n",
    "    loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    real_loss = loss_object(tf.ones_like(real_output), real_output) #if real, close to 1; if fake close to 0\n",
    "    generated_loss = loss_object(tf.zeros_like(generated_output), generated_output)\n",
    "    total_loss = real_loss + generated_loss\n",
    "    return total_loss * 0.5 #to prevent the discriminator from learning too quickly compared to the generator\n",
    "\n",
    " #Measures the L1 (absolute) difference between the original image and its reconstruction after a full cycle\n",
    " # to preserve the original paiting\n",
    "def calc_cycle_loss(real_image, cycled_image):\n",
    "    return tf.reduce_mean(tf.abs(real_image - cycled_image))\n",
    "\n",
    "#Regularization term to ensure that the generator does not produce images that are too different from the original image\n",
    "def identity_loss(real_image, same_image):\n",
    "    return tf.reduce_mean(tf.abs(real_image - same_image)) * LAMBDA * 0.5\n",
    "\n",
    "##############################\n",
    "# GETTING DATA #\n",
    "##############################\n",
    "# GETTING THE PRE-PROCESSED DATA\n",
    "def preprocess_image(image_file):\n",
    "    image = tf.io.read_file(image_file)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, [IMG_HEIGHT, IMG_WIDTH])\n",
    "    image = (tf.cast(image, tf.float32) / 127.5) - 1 \n",
    "    return image\n",
    "\n",
    "#Grabs one image at the time (batch size = 1), shuffles them, and preprocesses them one by one\n",
    "def create_dataset(image_paths, batch_size=1, shuffle=True):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(image_paths)\n",
    "    dataset = dataset.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(image_paths))\n",
    "    \n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset\n",
    "\n",
    "##############################\n",
    "# SIMPLE LATENT SPACE EXPLORER#\n",
    "##############################\n",
    "# I have more functions for latent space exploration but this was an initial and simple one - for more go to visualization notebook\n",
    "class LatentSpaceExplorer:\n",
    "    def __init__(self, generator_g, generator_f):\n",
    "        self.generator_g = generator_g  # painting -> OR\n",
    "        self.generator_f = generator_f  # OR -> painting\n",
    "    \n",
    "    def show_samples(self, sample_x, sample_y): #just shows a sample - more later\n",
    "        fake_y = self.generator_g(sample_x)\n",
    "        fake_x = self.generator_f(sample_y)\n",
    "        \n",
    "        def denormalize(image):\n",
    "            if isinstance(image, tf.Tensor):\n",
    "                image = image.numpy()\n",
    "            return (image + 1) * 0.5\n",
    "        \n",
    "        # Display images\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Original\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.imshow(denormalize(sample_x[0]))\n",
    "        plt.title('Original Painting')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.imshow(denormalize(sample_y[0]))\n",
    "        plt.title('Original OR Image')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Translations\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.imshow(denormalize(fake_y[0]))\n",
    "        plt.title('Painting → OR Image')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.imshow(denormalize(fake_x[0]))\n",
    "        plt.title('OR Image → Painting')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        #Mixed / Translated\n",
    "        plt.tight_layout()\n",
    "        plt.suptitle('CycleGAN Translations')\n",
    "        plt.show()\n",
    "    \n",
    "    def visualize_features(self, sample_image, generator, layer_idx=-4): #takes image, a generator, and a random layer - here i put 4\n",
    "   \n",
    "        layer = generator.layers[layer_idx]\n",
    "        feature_model = keras.Model(inputs=generator.input, outputs=layer.output)\n",
    "        features = feature_model(sample_image) #runs image through partial model to see intermediate \n",
    "        \n",
    "        # Plot feature maps\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Determine feature map dimensions\n",
    "        num_features = min(16, features.shape[-1])\n",
    "        rows = int(np.ceil(num_features / 4))\n",
    "        \n",
    "        for i in range(num_features):\n",
    "            plt.subplot(rows, 4, i+1)\n",
    "            \n",
    "            # feature map\n",
    "            feature_map = features[0, :, :, i]\n",
    "            if isinstance(feature_map, tf.Tensor):\n",
    "                feature_map = feature_map.numpy()\n",
    "            \n",
    "            # Normalize for visualization\n",
    "            feature_map = (feature_map - np.min(feature_map)) / (np.max(feature_map) - np.min(feature_map) + 1e-7)\n",
    "            \n",
    "            plt.imshow(feature_map, cmap='viridis')\n",
    "            plt.title(f'Feature {i+1}')\n",
    "            plt.axis('off')\n",
    "        \n",
    "        plt.suptitle(f'Feature Maps from Layer: {layer.name}')\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "        plt.show()\n",
    "\n",
    "##########################\n",
    "# !!Training CycleGAN !! #\n",
    "###########################\n",
    "def train_cyclegan(train_x_dataset, train_y_dataset, epochs=200, patience=20):\n",
    "    generator_g = build_generator()  # paintings to OR \n",
    "    generator_f = build_generator()  # OR to paintings\n",
    "    discriminator_x = build_discriminator() #if an image is a real painting or fake\n",
    "    discriminator_y = build_discriminator() #if an image is a real OR or fake\n",
    "    \n",
    "    # Optimizers\n",
    "    generator_g_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "    generator_f_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "    discriminator_x_optimizer = tf.keras.optimizers.Adam(1e-4, beta_1=0.5) #discriminator learning rates lower to prevent overpowering\n",
    "    discriminator_y_optimizer = tf.keras.optimizers.Adam(1e-4, beta_1=0.5)\n",
    "    \n",
    "    # Sample images for demonstration\n",
    "    sample_x = next(iter(train_x_dataset))\n",
    "    sample_y = next(iter(train_y_dataset))\n",
    "    \n",
    "    # For early stopping\n",
    "    best_loss = float('inf')\n",
    "    best_epoch = 0\n",
    "    no_improvement = 0\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        # Reset metrics for each epoch\n",
    "        n = 0\n",
    "        total_gen_g_loss = 0\n",
    "        total_gen_f_loss = 0\n",
    "        total_disc_x_loss = 0\n",
    "        total_disc_y_loss = 0\n",
    "        \n",
    "        for x_batch, y_batch in tf.data.Dataset.zip((train_x_dataset, train_y_dataset)):\n",
    "            losses = train_step(\n",
    "                x_batch, y_batch,\n",
    "                generator_g, generator_f,\n",
    "                discriminator_x, discriminator_y,\n",
    "                generator_g_optimizer, generator_f_optimizer,\n",
    "                discriminator_x_optimizer, discriminator_y_optimizer\n",
    "            )\n",
    "            \n",
    "            # Update loss totals\n",
    "            total_gen_g_loss += losses['gen_g_loss']\n",
    "            total_gen_f_loss += losses['gen_f_loss']\n",
    "            total_disc_x_loss += losses['disc_x_loss']\n",
    "            total_disc_y_loss += losses['disc_y_loss']\n",
    "            n += 1\n",
    "            \n",
    "            # Print progress every 5 batches\n",
    "            if n % 5 == 0:\n",
    "                print(f\"  Batch {n}: G loss: {losses['gen_g_loss']:.4f}, F loss: {losses['gen_f_loss']:.4f}\")\n",
    "        \n",
    "        # Calculate average losses for the epoch\n",
    "        avg_gen_g_loss = total_gen_g_loss/n\n",
    "        avg_gen_f_loss = total_gen_f_loss/n\n",
    "        avg_disc_x_loss = total_disc_x_loss/n\n",
    "        avg_disc_y_loss = total_disc_y_loss/n\n",
    "        \n",
    "        # Combined generator loss for early stopping\n",
    "        avg_gen_loss = (avg_gen_g_loss + avg_gen_f_loss) / 2\n",
    "        \n",
    "        # Early stopping check\n",
    "        if avg_gen_loss < best_loss:\n",
    "            best_loss = avg_gen_loss\n",
    "            best_epoch = epoch\n",
    "            no_improvement = 0\n",
    "            \n",
    "            # Save best models\n",
    "            print(f\"  New best model! Saving generators...\")\n",
    "            generator_g.save('best_generator_g.h5')\n",
    "            generator_f.save('best_generator_f.h5')\n",
    "        else:\n",
    "            no_improvement += 1\n",
    "            print(f\"  No improvement for {no_improvement} epochs. Best epoch: {best_epoch+1}\")\n",
    "        \n",
    "        # Epoch summary\n",
    "        print(f\"  Avg losses - G: {avg_gen_g_loss:.4f}, F: {avg_gen_f_loss:.4f}, D_X: {avg_disc_x_loss:.4f}, D_Y: {avg_disc_y_loss:.4f}\")\n",
    "        print(f\"  Time: {time.time()-start:.2f} sec\")\n",
    "        \n",
    "        # Generate example images every epoch\n",
    "        generate_images(epoch, generator_g, generator_f, sample_x, sample_y)\n",
    "        \n",
    "        # Check if we should stop\n",
    "        if no_improvement >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}. Best epoch was {best_epoch+1}.\")\n",
    "            \n",
    "            # Load best models\n",
    "            print(\"Loading best models...\")\n",
    "            generator_g = tf.keras.models.load_model('best_generator_g.h5')\n",
    "            generator_f = tf.keras.models.load_model('best_generator_f.h5')\n",
    "            break\n",
    "    \n",
    "    # Ensure we get back the best models, not the last ones\n",
    "    if no_improvement > 0 and os.path.exists('best_generator_g.h5'):\n",
    "        generator_g = tf.keras.models.load_model('best_generator_g.h5')\n",
    "        generator_f = tf.keras.models.load_model('best_generator_f.h5')\n",
    "    \n",
    "    return generator_g, generator_f\n",
    "\n",
    "def train_step(real_x, real_y, \n",
    "               generator_g, generator_f, \n",
    "               discriminator_x, discriminator_y,\n",
    "               generator_g_optimizer, generator_f_optimizer,\n",
    "               discriminator_x_optimizer, discriminator_y_optimizer):\n",
    "    \"\"\"Execute one training step\"\"\"\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        # Generate fake images\n",
    "        fake_y = generator_g(real_x, training=True)\n",
    "        fake_x = generator_f(real_y, training=True)\n",
    "        \n",
    "        # Cycle consistency - taking fake back to original\n",
    "        cycled_x = generator_f(fake_y, training=True)\n",
    "        cycled_y = generator_g(fake_x, training=True)\n",
    "        \n",
    "        # Identity mapping - paintings remain paintings and OR images remain OR images\n",
    "        same_x = generator_f(real_x, training=True)\n",
    "        same_y = generator_g(real_y, training=True)\n",
    "        \n",
    "        # Discriminator outputs\n",
    "        disc_real_x = discriminator_x(real_x, training=True)\n",
    "        disc_real_y = discriminator_y(real_y, training=True)\n",
    "        disc_fake_x = discriminator_x(fake_x, training=True)\n",
    "        disc_fake_y = discriminator_y(fake_y, training=True)\n",
    "        \n",
    "        # Generator losses\n",
    "        gen_g_loss = generator_loss(disc_fake_y)\n",
    "        gen_f_loss = generator_loss(disc_fake_x)\n",
    "        \n",
    "        # Cycle consistency losses\n",
    "        cycle_loss_x = calc_cycle_loss(real_x, cycled_x) * LAMBDA\n",
    "        cycle_loss_y = calc_cycle_loss(real_y, cycled_y) * LAMBDA\n",
    "        total_cycle_loss = cycle_loss_x + cycle_loss_y\n",
    "        \n",
    "        # Identity losses\n",
    "        id_loss_x = identity_loss(real_x, same_x)\n",
    "        id_loss_y = identity_loss(real_y, same_y)\n",
    "        total_id_loss = id_loss_x + id_loss_y\n",
    "        \n",
    "        # Total generator losses\n",
    "        total_gen_g_loss = gen_g_loss + total_cycle_loss + total_id_loss\n",
    "        total_gen_f_loss = gen_f_loss + total_cycle_loss + total_id_loss\n",
    "        \n",
    "        # Discriminator losses\n",
    "        disc_x_loss = discriminator_loss(disc_real_x, disc_fake_x)\n",
    "        disc_y_loss = discriminator_loss(disc_real_y, disc_fake_y)\n",
    "    \n",
    "    # Calculate gradients- gradients are calculated for each model and applied using their respective optimizers\n",
    "    generator_g_gradients = tape.gradient(total_gen_g_loss, generator_g.trainable_variables)\n",
    "    generator_f_gradients = tape.gradient(total_gen_f_loss, generator_f.trainable_variables)\n",
    "    discriminator_x_gradients = tape.gradient(disc_x_loss, discriminator_x.trainable_variables)\n",
    "    discriminator_y_gradients = tape.gradient(disc_y_loss, discriminator_y.trainable_variables)\n",
    "    \n",
    "    # Apply gradients from above\n",
    "    generator_g_optimizer.apply_gradients(zip(generator_g_gradients, generator_g.trainable_variables))\n",
    "    generator_f_optimizer.apply_gradients(zip(generator_f_gradients, generator_f.trainable_variables))\n",
    "    discriminator_x_optimizer.apply_gradients(zip(discriminator_x_gradients, discriminator_x.trainable_variables))\n",
    "    discriminator_y_optimizer.apply_gradients(zip(discriminator_y_gradients, discriminator_y.trainable_variables))\n",
    "    \n",
    "    return {\n",
    "        'gen_g_loss': gen_g_loss,\n",
    "        'gen_f_loss': gen_f_loss,\n",
    "        'disc_x_loss': disc_x_loss,\n",
    "        'disc_y_loss': disc_y_loss\n",
    "    }\n",
    "\n",
    "def generate_images(epoch, generator_g, generator_f, test_input_x, test_input_y):\n",
    "    if not os.path.exists('images'):\n",
    "        os.makedirs('images')\n",
    "    \n",
    "    # Generate images\n",
    "    prediction_y = generator_g(test_input_x)\n",
    "    prediction_x = generator_f(test_input_y)\n",
    "    \n",
    "    # Function to denormalize images\n",
    "    def denormalize(img):\n",
    "        if isinstance(img, tf.Tensor):\n",
    "            img = img.numpy()\n",
    "        return (img + 1) * 0.5\n",
    "    \n",
    "    # Create figure\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    \n",
    "    display_list = [\n",
    "        denormalize(test_input_x[0]),\n",
    "        denormalize(prediction_y[0]),\n",
    "        denormalize(test_input_y[0]),\n",
    "        denormalize(prediction_x[0])\n",
    "    ]\n",
    "    \n",
    "    title = ['Input Painting', 'Painting → OR Image', 'Input OR Image', 'OR Image → Painting']\n",
    "    \n",
    "    for i in range(4):\n",
    "        plt.subplot(2, 2, i+1)\n",
    "        plt.title(title[i])\n",
    "        plt.imshow(display_list[i])\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'images/epoch_{epoch+1}.png')\n",
    "    plt.close()\n",
    "\n",
    "# RUN CYCLE GAN + SIMPLE LATENT SPACE EXPLORATION\n",
    "def run_cyclegan(train_paintings, train_or_images, epochs=5):\n",
    "    train_paintings_ds = create_dataset(train_paintings, batch_size=1)\n",
    "    train_or_images_ds = create_dataset(train_or_images, batch_size=1)\n",
    "    \n",
    "    # Train model\n",
    "    print(f\"Training CycleGAN for {epochs} epochs...\")\n",
    "    generator_g, generator_f = train_cyclegan(\n",
    "        train_paintings_ds, train_or_images_ds, \n",
    "        epochs=epochs\n",
    "    )\n",
    "    \n",
    "    # Create latent space explorer\n",
    "    print(\"Exploring latent space:\")\n",
    "    explorer = LatentSpaceExplorer(generator_g, generator_f)\n",
    "    \n",
    "    # Show sample images\n",
    "    sample_x = next(iter(train_paintings_ds))\n",
    "    sample_y = next(iter(train_or_images_ds))\n",
    "    explorer.show_samples(sample_x, sample_y)\n",
    "    \n",
    "    # Visualize features\n",
    "    print(\"Generator features:\")\n",
    "    explorer.visualize_features(sample_x, generator_g)\n",
    "    \n",
    "    return generator_g, generator_f, explorer\n",
    "\n",
    "# Execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Define paths to the preprocessed datasets\n",
    "    train_paintings = [os.path.join(\"preprocessed_paintings/train\", f) for f in os.listdir(\"preprocessed_paintings/train\") if f.endswith('.jpg')]\n",
    "    test_paintings = [os.path.join(\"preprocessed_paintings/test\", f) for f in os.listdir(\"preprocessed_paintings/test\") if f.endswith('.jpg')]\n",
    "    \n",
    "    train_or_images = [os.path.join(\"preprocessed_or_images/train\", f) for f in os.listdir(\"preprocessed_or_images/train\") if f.endswith('.jpg')]\n",
    "    test_or_images = [os.path.join(\"preprocessed_or_images/test\", f) for f in os.listdir(\"preprocessed_or_images/test\") if f.endswith('.jpg')]\n",
    "    \n",
    "    # Print found files to verify\n",
    "    print(f\"Found {len(train_paintings)} training paintings\")\n",
    "    print(f\"Found {len(train_or_images)} training OR images\")\n",
    "    print(f\"Found {len(test_paintings)} test paintings\")\n",
    "    print(f\"Found {len(test_or_images)} test OR images\")\n",
    "    \n",
    "    # Run CycleGAN\n",
    "    generator_g, generator_f, explorer = run_cyclegan(\n",
    "        train_paintings, \n",
    "        train_or_images, \n",
    "        epochs=2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Style Transfer Strength Control\n",
    "# This code assumes you've already trained your CycleGAN model\n",
    "# and have generator_g, generator_f available\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def visualize_style_transfer_strength(generator, input_image, steps=8):\n",
    "    \"\"\"\n",
    "    Visualize the effect of gradually increasing style transfer strength\n",
    "    by blending between the original image and the fully translated image\n",
    "    \"\"\"\n",
    "    # Ensure input is a batch\n",
    "    if len(input_image.shape) == 3:\n",
    "        input_image = tf.expand_dims(input_image, 0)\n",
    "    \n",
    "    # Generate the translated image\n",
    "    translated_image = generator(input_image)\n",
    "    \n",
    "    # Create blends with different strengths\n",
    "    blended_images = []\n",
    "    for alpha in np.linspace(0, 1, steps):\n",
    "        # Linear interpolation between original and translated\n",
    "        blended = (1 - alpha) * input_image + alpha * translated_image\n",
    "        blended_images.append(blended[0])\n",
    "    \n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(16, 4))\n",
    "    \n",
    "    # Function to convert from normalized [-1, 1] to display range [0, 1]\n",
    "    def denormalize(img):\n",
    "        if isinstance(img, tf.Tensor):\n",
    "            img = img.numpy()\n",
    "        return (img + 1) * 0.5\n",
    "    \n",
    "    # Display each blend\n",
    "    for i, img in enumerate(blended_images):\n",
    "        plt.subplot(1, steps, i+1)\n",
    "        plt.imshow(denormalize(img))\n",
    "        plt.title(f'Strength: {i/(steps-1):.2f}')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.suptitle('Style Transfer Strength Control')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return blended_images\n",
    "\n",
    "# Helper function to create a dataset from image paths\n",
    "def create_dataset(image_paths, batch_size=1, shuffle=True, img_height=256, img_width=256):\n",
    "    \"\"\"Create a TensorFlow dataset from image paths\"\"\"\n",
    "    def preprocess_image(image_file):\n",
    "        \"\"\"Load and preprocess an image from file path\"\"\"\n",
    "        image = tf.io.read_file(image_file)\n",
    "        image = tf.image.decode_jpeg(image, channels=3)\n",
    "        image = tf.image.resize(image, [img_height, img_width])\n",
    "        image = (tf.cast(image, tf.float32) / 127.5) - 1  # Normalize to [-1, 1]\n",
    "        return image\n",
    "        \n",
    "    dataset = tf.data.Dataset.from_tensor_slices(image_paths)\n",
    "    dataset = dataset.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(image_paths))\n",
    "    \n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset\n",
    "\n",
    "# Example usage: \n",
    "# Visualize painting to OR image translation with varying strength\n",
    "def show_style_transfer_strength_demo(generator_g, generator_f, train_paintings_ds, train_or_images_ds):\n",
    "    sample_painting = next(iter(train_paintings_ds))\n",
    "    sample_or_image = next(iter(train_or_images_ds))\n",
    "    \n",
    "    print(\"Painting → OR Image with varying strength:\")\n",
    "    painting_to_or = visualize_style_transfer_strength(generator_g, sample_painting)\n",
    "    \n",
    "    print(\"\\nOR Image → Painting with varying strength:\")\n",
    "    or_to_painting = visualize_style_transfer_strength(generator_f, sample_or_image)\n",
    "    \n",
    "    return painting_to_or, or_to_painting\n",
    "\n",
    "# This section should be run after your GAN has been trained\n",
    "# First, create the datasets\n",
    "train_paintings = [os.path.join(\"preprocessed_paintings/train\", f) for f in os.listdir(\"preprocessed_paintings/train\") if f.endswith('.jpg')]\n",
    "train_or_images = [os.path.join(\"preprocessed_or_images/train\", f) for f in os.listdir(\"preprocessed_or_images/train\") if f.endswith('.jpg')]\n",
    "\n",
    "# Create the datasets\n",
    "train_paintings_ds = create_dataset(train_paintings, batch_size=1)\n",
    "train_or_images_ds = create_dataset(train_or_images, batch_size=1)\n",
    "\n",
    "# Now run the visualization\n",
    "# Assuming generator_g and generator_f are already defined from your training\n",
    "painting_to_or, or_to_painting = show_style_transfer_strength_demo(generator_g, generator_f, train_paintings_ds, train_or_images_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Progressive Generation Visualization\n",
    "# This code shows how an image progresses through the generator network\n",
    "\n",
    "def visualize_progressive_generation(generator, input_image):\n",
    "    \"\"\"\n",
    "    Visualize how an image transforms through the main layers of the generator\n",
    "    \"\"\"\n",
    "    # Ensure input is a batch\n",
    "    if len(input_image.shape) == 3:\n",
    "        input_image = tf.expand_dims(input_image, 0)\n",
    "    \n",
    "    # Create models for various depths of the generator\n",
    "    intermediate_models = []\n",
    "    \n",
    "    # Find key layers to visualize (downsampling, bottleneck, and upsampling)\n",
    "    key_indices = []\n",
    "    bottleneck_found = False\n",
    "    \n",
    "    for i, layer in enumerate(generator.layers):\n",
    "        # Skip the input layer\n",
    "        if i == 0:\n",
    "            continue\n",
    "        \n",
    "        # Look for significant layers like Conv2D, Conv2DTranspose, and Add\n",
    "        layer_type = layer.__class__.__name__\n",
    "        if layer_type in ['Conv2D', 'Conv2DTranspose', 'Add']:\n",
    "            # Mark bottleneck (smallest spatial dimension)\n",
    "            if not bottleneck_found and i > len(generator.layers) // 3:\n",
    "                bottleneck_found = True\n",
    "                key_indices.append(i)\n",
    "            # Add a few key layers from downsampling and upsampling\n",
    "            elif i % 4 == 0:  # Sample roughly every 4th significant layer\n",
    "                key_indices.append(i)\n",
    "    \n",
    "    # Add the final output layer\n",
    "    key_indices.append(len(generator.layers) - 1)\n",
    "    \n",
    "    # Limit to at most 8 layers to avoid overcrowding\n",
    "    if len(key_indices) > 8:\n",
    "        # Ensure we keep first, bottleneck, and last\n",
    "        essential_indices = [key_indices[0], key_indices[len(key_indices)//2], key_indices[-1]]\n",
    "        remaining = 5  # 8 - 3 essential indices\n",
    "        \n",
    "        # Select additional indices evenly distributed\n",
    "        step = (len(key_indices) - 3) // (remaining + 1)\n",
    "        selected_indices = []\n",
    "        \n",
    "        for i in range(1, remaining + 1):\n",
    "            idx = i * step\n",
    "            if idx < len(key_indices) - 1:  # Avoid the last index as it's already in essential_indices\n",
    "                selected_indices.append(key_indices[idx])\n",
    "        \n",
    "        key_indices = sorted(essential_indices + selected_indices)\n",
    "    \n",
    "    # Create models for each key layer\n",
    "    for idx in key_indices:\n",
    "        layer = generator.layers[idx]\n",
    "        intermediate_model = tf.keras.Model(inputs=generator.input, outputs=layer.output)\n",
    "        intermediate_models.append((idx, layer.name, intermediate_model))\n",
    "    \n",
    "    # Generate intermediate outputs\n",
    "    intermediate_outputs = []\n",
    "    for idx, name, model in intermediate_models:\n",
    "        output = model(input_image)\n",
    "        intermediate_outputs.append((idx, name, output))\n",
    "    \n",
    "    # Plot the results\n",
    "    num_outputs = len(intermediate_outputs)\n",
    "    rows = int(np.ceil(num_outputs / 4))\n",
    "    cols = min(num_outputs, 4)\n",
    "    \n",
    "    plt.figure(figsize=(16, 4 * rows))\n",
    "    \n",
    "    # Function to convert from normalized [-1, 1] to display range [0, 1]\n",
    "    def denormalize(img):\n",
    "        if isinstance(img, tf.Tensor):\n",
    "            img = img.numpy()\n",
    "        return (img + 1) * 0.5\n",
    "    \n",
    "    # Display input image\n",
    "    plt.subplot(rows, cols, 1)\n",
    "    plt.imshow(denormalize(input_image[0]))\n",
    "    plt.title('Input Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Display intermediate outputs\n",
    "    for i, (idx, name, output) in enumerate(intermediate_outputs[1:], start=2):\n",
    "        plt.subplot(rows, cols, i)\n",
    "        \n",
    "        # Handle different output shapes\n",
    "        if len(output.shape) == 4:\n",
    "            # For feature maps, use the first 3 channels combined as RGB\n",
    "            if output.shape[-1] > 3:\n",
    "                # Take first 3 channels\n",
    "                channels = output[0, :, :, :3]\n",
    "                # Normalize each channel independently\n",
    "                channels_normalized = tf.stack([\n",
    "                    (channels[:, :, i] - tf.reduce_min(channels[:, :, i])) / \n",
    "                    (tf.reduce_max(channels[:, :, i]) - tf.reduce_min(channels[:, :, i]) + 1e-7)\n",
    "                    for i in range(3)\n",
    "                ], axis=-1)\n",
    "                plt.imshow(channels_normalized)\n",
    "            else:\n",
    "                # If exactly 3 channels, treat as RGB\n",
    "                plt.imshow(denormalize(output[0]))\n",
    "        else:\n",
    "            # For 2D outputs, use grayscale\n",
    "            plt.imshow(output[0], cmap='gray')\n",
    "        \n",
    "        plt.title(f'Layer {idx}: {name}')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.suptitle('Progressive Generation Through Generator Layers')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return intermediate_outputs\n",
    "\n",
    "# Function to showcase progressive generation for both generators\n",
    "def show_progressive_generation_demo(generator_g, generator_f, train_paintings_ds, train_or_images_ds):\n",
    "    # Get sample images\n",
    "    sample_painting = next(iter(train_paintings_ds))\n",
    "    sample_or_image = next(iter(train_or_images_ds))\n",
    "    \n",
    "    # Show progression for painting to OR image\n",
    "    print(\"Progression: Painting → OR Image\")\n",
    "    painting_progression = visualize_progressive_generation(generator_g, sample_painting)\n",
    "    \n",
    "    # Show progression for OR image to painting\n",
    "    print(\"\\nProgression: OR Image → Painting\")\n",
    "    or_progression = visualize_progressive_generation(generator_f, sample_or_image)\n",
    "    \n",
    "    return painting_progression, or_progression\n",
    "\n",
    "# Run this after training\n",
    "# painting_progression, or_progression = show_progressive_generation_demo(generator_g, generator_f, train_paintings_ds, train_or_images_ds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
